{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#=====================\u52a0\u8f7d\u8981\u7528\u7684\u6a21\u5757======================#\n",
      "from __future__ import print_function\n",
      "#\u4fdd\u8bc1python3.x\u7684\u517c\u5bb9\u6027\n",
      "#import os\n",
      "import numpy as np\n",
      "#import random\n",
      "#import string\n",
      "import tensorflow as tf\n",
      "#\u5bfc\u5165\u753b\u56fe\u6a21\u5757pyplot\n",
      "import matplotlib.pyplot as plt\n",
      "#\u5bfc\u5165\u6df7\u6dc6\u77e9\u9635\u6a21\u5757\n",
      "from sklearn.metrics import classification_report\n",
      "#=====================\u52a0\u8f7d\u8981\u7528\u7684\u6a21\u5757======================#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
        "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#=============\u8bfb\u5165\u6570\u636e===============#\n",
      "####\u5c06\u53e5\u5b50\u7684\u7d22\u5f15\u5f62\u5f0f\u8bfb\u5165\u4e3anp.array\u7684\u5f62\u5f0f####\n",
      "#\u8f93\u5165\uff1a\u53e5\u5b50\u7d22\u5f15\u7684\u6587\u4ef6\u540d\uff08\u8def\u5f84+\u6587\u4ef6\u540d\uff09\uff0c\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\uff08int\uff09\uff0c\u53e5\u5b50\u6700\u5927\u957f\u5ea6\u4f7f\u80fd\uff08bool\uff09\n",
      "#\u8f93\u51fa\uff1a\u6269\u5145\u6216\u526a\u6389\u7684\u53e5\u5b50\u7d22\u5f15\u6570\u5217\uff08np.array\uff09\uff0c\u8868\u660e\u90a3\u4e9b\u4f4d\u7f6e\u662f\u6269\u5145\u51fa\u6765\u7684mask\u6570\u5217\uff08np.array\uff09\n",
      "def readSentence2Array(sentenceFileName,inputMaxLength = 16 , inputMaxLengthFlag = False):\n",
      "    file = open(sentenceFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    sentenceList = []\n",
      "    mask = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip().split(' ')\n",
      "        sentenceList.append(splitList)\n",
      "        mask.append([1.0]*len(splitList))\n",
      "    maxLength = len(sentenceList[0])\n",
      "    for row in sentenceList:\n",
      "        if maxLength <= len(row):\n",
      "            maxLength = len(row)\n",
      "    if inputMaxLengthFlag == False: \n",
      "        for row in sentenceList:\n",
      "            for i in range(len(row),maxLength):\n",
      "                row.append('0')\n",
      "        for row in mask:\n",
      "            for i in range(len(row),maxLength):\n",
      "                row.append(0.0)\n",
      "                \n",
      "    elif inputMaxLengthFlag == True:\n",
      "        temp_list = []\n",
      "        for row in sentenceList:\n",
      "            if len(row) < inputMaxLength:\n",
      "                for i in range(len(row),inputMaxLength):\n",
      "                    row.append('0')\n",
      "                temp_list.append(row)\n",
      "            elif len(row) >= inputMaxLength:\n",
      "                temp_list.append(row[0:inputMaxLength])\n",
      "        sentenceList = temp_list\n",
      "        \n",
      "        temp_mask = []\n",
      "        for row in mask:\n",
      "            if len(row) < inputMaxLength:\n",
      "                for i in range(len(row),inputMaxLength):\n",
      "                    row.append(0.0)\n",
      "                temp_mask.append(row)\n",
      "            elif len(row) >= inputMaxLength:\n",
      "                temp_mask.append(row[0:inputMaxLength])\n",
      "        mask = temp_mask\n",
      "                \n",
      "                \n",
      "        \n",
      "        \n",
      "    numSentenceList = strList2numList2D(sentenceList)\n",
      "    print (np.array(numSentenceList))\n",
      "    print (np.size(np.array(mask),0))\n",
      "    print (np.size(np.array(mask),1))\n",
      "    print (np.array(mask))\n",
      "    \n",
      "    return np.array(numSentenceList),np.array(mask)\n",
      "\n",
      "####\u5c06\u5b57\u7b26\u7c7b\u578b\u7684list\u8f6c\u6362\u4e3a\u6570\u5b57\u7c7b\u578b\u7684list\uff082D\uff09####\n",
      "#\u8f93\u5165\uff1a\u5b57\u7b26\u7c7b\u578b\u7684list\n",
      "#\u8f93\u51fa\uff1a\u5bf9\u5e94\u7684\u6570\u5b57\u7c7b\u578b\u7684list\n",
      "def strList2numList2D(strlist):\n",
      "    copyList = []\n",
      "    for sentence in strlist:\n",
      "        temp_list = []\n",
      "        for number in sentence:\n",
      "            temp_list.append(int(number))\n",
      "        copyList.append(temp_list)\n",
      "    return copyList\n",
      "\n",
      "####\u8bfb\u5165label\u6587\u4ef6####\n",
      "#\u8f93\u5165\uff1alabel\u6587\u4ef6\u7684\u6587\u4ef6\u540d\n",
      "#\u8f93\u51fa\uff1alabel\u6570\u5217\uff08np.array\uff09\n",
      "def readLabelFile2Array(LabelFileName):\n",
      "    file = open(LabelFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    labelList = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip()\n",
      "        labelList.append(splitList)\n",
      "    return np.array(labelList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u52a0\u8f7dembeddings\n",
      "savedArrayFile =r'/media/huxi/\u65b0\u52a0\u53771/SCI_aim/experiment/code/sswe_ver/goal_data_label/vocabularyEmbedding.npy'\n",
      "wordEmbedding = np.load(savedArrayFile)\n",
      "\n",
      "lengthEmbedding = np.shape(wordEmbedding)[1]#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary = np.shape(wordEmbedding)[0]#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "\n",
      "sentenceFileName = r'/media/huxi/\u65b0\u52a0\u53771/SCI_aim/experiment/code/sswe_ver/goal_data_label/goal_data.txt'\n",
      "sentenceArray,maskArray = readSentence2Array(sentenceFileName)\n",
      "num_array = np.shape(sentenceArray)[0]#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength = np.shape(sentenceArray)[1]#\u53e5\u5b50\u957f\u5ea6\n",
      "\n",
      "\n",
      "\n",
      "#\u52a0\u8f7dlabel\u6587\u4ef6\n",
      "LabelFileName = r'/media/huxi/\u65b0\u52a0\u53771/SCI_aim/experiment/code/sswe_ver/goal_data_label/goal_label.txt'\n",
      "labels = readLabelFile2Array(LabelFileName)\n",
      "\n",
      "#\u5c06dataset\u548clabel\u5206\u62108\uff1a1\uff1a1\u7684\u6bd4\u4f8b\n",
      "train_dataset = sentenceArray[0:int(num_array*0.8)]\n",
      "valid_dataset = sentenceArray[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_dataset = sentenceArray[int(num_array*0.8)+int(num_array*0.1):]\n",
      "train_labels = labels[0:int(num_array*0.8)]\n",
      "valid_labels = labels[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_labels = labels[int(num_array*0.8)+int(num_array*0.1):]\n",
      "\n",
      "train_mask = maskArray[0:int(num_array*0.8)]\n",
      "valid_mask = maskArray[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_mask = maskArray[int(num_array*0.8)+int(num_array*0.1):]\n",
      "\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[4318 2538 4132 ...,    0    0    0]\n",
        " [7032 3485 3214 ...,    0    0    0]\n",
        " [3641 7350 8091 ...,    0    0    0]\n",
        " ..., \n",
        " [1117 1434  318 ...,    0    0    0]\n",
        " [8674 9132 3369 ...,    0    0    0]\n",
        " [5675 6906 3151 ...,    0    0    0]]\n",
        "6000\n",
        "23\n",
        "[[ 1.  1.  1. ...,  0.  0.  0.]\n",
        " [ 1.  1.  1. ...,  0.  0.  0.]\n",
        " [ 1.  1.  1. ...,  0.  0.  0.]\n",
        " ..., \n",
        " [ 1.  1.  1. ...,  0.  0.  0.]\n",
        " [ 1.  1.  1. ...,  0.  0.  0.]\n",
        " [ 1.  1.  1. ...,  0.  0.  0.]]\n",
        "Training set (4800, 23) (4800,)\n",
        "Validation set (600, 23) (600,)\n",
        "Test set (600, 23) (600,)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#\u8f93\u5165\uff1a\u76f4\u63a5\u8bfb\u51fa\u7684dataset\u6570\u5217\uff0c\u76f4\u63a5\u8bfb\u51fa\u7684label\u6570\u5217\n",
      "#\u8f93\u51fa\uff1a\u91cd\u6784\u540e\u7684dataset\u6570\u5217\uff08np.array\uff09\uff0clabel\u6570\u5217\uff08np.array\uff09\n",
      "def reformat(dataset,labels):\n",
      "  kindOfLabels = list(set(labels))\n",
      "  reformedLabels = []\n",
      "  for label in labels:\n",
      "        reformedLabels.append(kindOfLabels.index(label))\n",
      "  reformedLabels = np.array(reformedLabels)\n",
      "  return dataset,reformedLabels\n",
      "    \n",
      "#\u8fd0\u884c\u51fd\u6570\n",
      "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
      "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
      "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set (4800, 23) (4800,)\n",
        "Validation set (600, 23) (600,)\n",
        "Test set (600, 23) (600,)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####\u6839\u636e\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4e0elabels\u6bd4\u8f83\uff0c\u5f97\u51fa\u51c6\u786e\u7387####\n",
      "#\u8f93\u5165\uff1a\u9884\u6d4b\u7ed3\u679c\uff08one hot array\uff09\uff0clabels\uff08\u4e00\u7ef4 array\uff09\n",
      "#\u8f93\u51fa\uff1a\u51c6\u786e\u7387\n",
      "def accuracy(predictions, labels):\n",
      "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
      "          /predictions.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_values(pc, fmt=\"%.2f\", **kw):\n",
      "    '''\n",
      "    Heatmap with text in each cell with matplotlib's pyplot\n",
      "    Source: http://stackoverflow.com/a/25074150/395857 \n",
      "    By HYRY\n",
      "    '''\n",
      "    from itertools import izip\n",
      "    pc.update_scalarmappable()\n",
      "    ax = pc.get_axes()\n",
      "    for p, color, value in izip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
      "        x, y = p.vertices[:-2, :].mean(0)\n",
      "        if np.all(color[:3] > 0.5):\n",
      "            color = (0.0, 0.0, 0.0)\n",
      "        else:\n",
      "            color = (1.0, 1.0, 1.0)\n",
      "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
      "\n",
      "\n",
      "def cm2inch(*tupl):\n",
      "    '''\n",
      "    Specify figure size in centimeter in matplotlib\n",
      "    Source: http://stackoverflow.com/a/22787457/395857\n",
      "    By gns-ank\n",
      "    '''\n",
      "    inch = 2.54\n",
      "    if type(tupl[0]) == tuple:\n",
      "        return tuple(i/inch for i in tupl[0])\n",
      "    else:\n",
      "        return tuple(i/inch for i in tupl)\n",
      "\n",
      "\n",
      "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
      "    '''\n",
      "    Inspired by:\n",
      "    - http://stackoverflow.com/a/16124677/395857 \n",
      "    - http://stackoverflow.com/a/25074150/395857\n",
      "    '''\n",
      "\n",
      "    # Plot it out\n",
      "    fig, ax = plt.subplots()    \n",
      "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
      "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
      "\n",
      "    # put the major ticks at the middle of each cell\n",
      "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
      "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
      "\n",
      "    # set tick labels\n",
      "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
      "    ax.set_xticklabels(xticklabels, minor=False)\n",
      "    ax.set_yticklabels(yticklabels, minor=False)\n",
      "\n",
      "    # set title and x/y labels\n",
      "    plt.title(title)\n",
      "    plt.xlabel(xlabel)\n",
      "    plt.ylabel(ylabel)      \n",
      "\n",
      "    # Remove last blank column\n",
      "    plt.xlim( (0, AUC.shape[1]) )\n",
      "\n",
      "    # Turn off all the ticks\n",
      "    ax = plt.gca()    \n",
      "    for t in ax.xaxis.get_major_ticks():\n",
      "        t.tick1On = False\n",
      "        t.tick2On = False\n",
      "    for t in ax.yaxis.get_major_ticks():\n",
      "        t.tick1On = False\n",
      "        t.tick2On = False\n",
      "\n",
      "    # Add color bar\n",
      "    plt.colorbar(c)\n",
      "\n",
      "    # Add text in each cell \n",
      "    show_values(c)\n",
      "\n",
      "    # Proper orientation (origin at the top left instead of bottom left)\n",
      "    if correct_orientation:\n",
      "        ax.invert_yaxis()\n",
      "        ax.xaxis.tick_top()       \n",
      "\n",
      "    # resize \n",
      "    fig = plt.gcf()\n",
      "    #fig.set_size_inches(cm2inch(40, 20))\n",
      "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
      "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
      "\n",
      "\n",
      "\n",
      "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
      "    '''\n",
      "    Plot scikit-learn classification report.\n",
      "    Extension based on http://stackoverflow.com/a/31689645/395857 \n",
      "    '''\n",
      "    lines = classification_report.split('\\n')\n",
      "\n",
      "    classes = []\n",
      "    plotMat = []\n",
      "    support = []\n",
      "    class_names = []\n",
      "    for line in lines[2 : (len(lines) - 2)]:\n",
      "        t = line.strip().split()\n",
      "        if len(t) < 2: continue\n",
      "        classes.append(t[0])\n",
      "        v = [float(x) for x in t[1: len(t) - 1]]\n",
      "        support.append(int(t[-1]))\n",
      "        class_names.append(t[0])\n",
      "        print(v)\n",
      "        plotMat.append(v)\n",
      "\n",
      "    print('plotMat: {0}'.format(plotMat))\n",
      "    print('support: {0}'.format(support))\n",
      "\n",
      "    xlabel = 'Metrics'\n",
      "    ylabel = 'Classes'\n",
      "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
      "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
      "    figure_width = 25\n",
      "    figure_height = len(class_names) + 7\n",
      "    correct_orientation = False\n",
      "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####LSTM\u6a21\u578b####\n",
      "#\u5728\u6bcf\u4e00\u4e2alstm cell\u4e2d\u7684hidden unit\u7684\u6570\u91cf\n",
      "num_nodes = 512+64\n",
      "\n",
      "#label\u7684\u7c7b\u522b\u6570\uff0c\u4e5f\u662f\u5206\u7c7b\u6570\n",
      "kindNumLabels = len(list(set(labels)))\n",
      "\n",
      "#\u8bbe\u4e3a\u6700\u5927\u53e5\u5b50\u957f\u5ea6\n",
      "num_unrollings = 23\n",
      "\n",
      "#\u4e00\u4e2abatch\u7684\u5927\u5c0f\n",
      "batch_size = 128\n",
      "\n",
      "dropout_keep = 0.5\n",
      "'''\n",
      "wordEmbedding#\u6240\u6709\u8bcd\u5411\u91cf\n",
      "lengthEmbedding#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "sentenceArray#\u53e5\u5b50\u5411\u91cf\n",
      "num_array#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength#\u53e5\u5b50\u957f\u5ea6\n",
      "'''\n",
      "\n",
      "\n",
      "embedding_size = lengthEmbedding#\u8bcd\u5411\u91cf\u7ef4\u6570\n",
      "\n",
      "#\u5b9a\u4e49\u8ba1\u7b97\u56fe\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "  #\u8bcd\u5411\u91cf\u521d\u59cb\u5316\u4e3apre-train\u8fc7\u7684\u8bcd\u5411\u91cf\n",
      "  embeddings = tf.Variable(wordEmbedding,dtype=tf.float32)\n",
      "  \n",
      "  # Parameters:\n",
      "  # Input gate: input, previous output, and bias.\n",
      "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Forget gate: input, previous output, and bias.\n",
      "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Memory cell: input, state and bias.                             \n",
      "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Output gate: input, previous output, and bias.\n",
      "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Variables saving state across unrollings.\n",
      "  train_saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "  train_saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    \n",
      "  valid_saved_output = tf.Variable(tf.zeros([valid_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  valid_saved_state = tf.Variable(tf.zeros([valid_dataset.shape[0], num_nodes]), trainable=False)\n",
      "    \n",
      "  test_saved_output = tf.Variable(tf.zeros([test_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  test_saved_state = tf.Variable(tf.zeros([test_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  # Classifier weights and biases.\n",
      "  w = tf.Variable(tf.truncated_normal([num_nodes, kindNumLabels], -0.1, 0.1))\n",
      "  b = tf.Variable(tf.zeros([kindNumLabels]))  \n",
      "    \n",
      "  # Definition of the cell computation.\n",
      "  def lstm_cell(i, o, state,dropoutFlag):\n",
      "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
      "    Note that in this formulation, we omit the various connections between the\n",
      "    previous state and the gates.\"\"\"\n",
      "    embed = tf.nn.embedding_lookup(embeddings, i)#1\u00d7embedding_size \u8fd9\u91cc\u7684i\u9700\u8981\u662f\u4e00\u4e2aint32\u6216int64\n",
      "    #print (embed)\n",
      "    \n",
      "    input_gate = tf.sigmoid(tf.matmul(embed, ix) + tf.matmul(o, im) + ib)#i\u4e3a\u8f93\u5165\u7684x,o\u4e3a\u4e0a\u4e00\u4e2acell\u4e2d\u4f20\u6765\u7684h\n",
      "    forget_gate = tf.sigmoid(tf.matmul(embed, fx) + tf.matmul(o, fm) + fb)\n",
      "    update = tf.matmul(embed, cx) + tf.matmul(o, cm) + cb\n",
      "    \n",
      "    if dropoutFlag == True:\n",
      "        update = tf.nn.dropout(update,dropout_keep)\n",
      "        \n",
      "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "    output_gate = tf.sigmoid(tf.matmul(embed, ox) + tf.matmul(o, om) + ob)\n",
      "    return output_gate * tf.tanh(state), state\n",
      "  '''\n",
      "  #==========\u8bd5\u8bd5\u80fd\u4e0d\u80fd\u5199\u4e2amodel\u51fa\u6765=============\n",
      "  '''\n",
      "  def model(data,mask,chooseState=\"train\",dropoutFlag = False):\n",
      "      # Unrolled LSTM loop.\n",
      "        \n",
      "      outputs = list()\n",
      "      if chooseState==\"train\":\n",
      "            saved_output = train_saved_output\n",
      "            saved_state = train_saved_state\n",
      "      elif chooseState==\"valid\":\n",
      "            saved_output = valid_saved_output\n",
      "            saved_state = valid_saved_state\n",
      "      elif chooseState==\"test\":\n",
      "            saved_output = test_saved_output\n",
      "            saved_state = test_saved_state\n",
      "            \n",
      "      output = saved_output\n",
      "      state = saved_state\n",
      "      for i in data:\n",
      "        output, state = lstm_cell(i, output, state,dropoutFlag)\n",
      "        outputs.append(output)\n",
      "        \n",
      "      masks = list() \n",
      "      for i in mask:\n",
      "        masks.append(i)\n",
      "      expand_masks = tf.expand_dims(tf.pack(mask), -1) #\u5c06masks\u62d3\u5c55\u4e00\u4e0b\uff0c\u4f7f\u5f97\u53ef\u4ee5\u548coutputs\u80fd\u5bf9\u4e0a :expand_masks [128, 32, 1]\n",
      "      masked_outputs = tf.pack(outputs)*expand_masks #\u5c06mask\u6389\u7684\u8f93\u51fa\u9879\u7ed9\u5c4f\u853d\u6389 masked_outputs:[128, 32, 64]\n",
      "      mask_sum = tf.reduce_sum(expand_masks, 0) #\u5224\u65ad\u4e00\u4e2a\u53e5\u5b50\u4e2d\u6709\u591a\u5c11\u4e2a\u8bcd\u6ca1\u88ab\u5c4f\u853d:[32,64]\n",
      "      pooling = tf.reduce_sum(masked_outputs, 0)/mask_sum #:[32,64] \n",
      "      #output\u4e3a[batch_size, num_nodes]\uff0coutputs\u91cc\u6bcf\u4e00\u9879\u90fd\u662f\u8fd9\u4e2asize\n",
      "      #pooling = tf.reduce_mean(tf.pack(outputs), 0)\n",
      "      #pooling\u4e4b\u540e\u5e94\u8be5\u662f\uff08batch_size\uff0cnum_nodes\uff09\n",
      "      if chooseState==\"train\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([train_saved_output.assign(output),\n",
      "                                        train_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      elif chooseState==\"valid\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([valid_saved_output.assign(output),\n",
      "                                        valid_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      elif chooseState==\"test\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([test_saved_output.assign(output),\n",
      "                                        test_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      return logits\n",
      "  '''\n",
      "  ==============================================\n",
      "  '''\n",
      "\n",
      "  tf_train_inputs = list()\n",
      "  tf_train_mask = list()\n",
      "  for _ in range(num_unrollings):\n",
      "    tf_train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size]))#[batch_size,vocabulary_size]))\n",
      "    tf_train_mask.append(tf.placeholder(tf.float32, shape=[batch_size]))\n",
      "  tf_train_labels = tf.placeholder(tf.int64, shape=[batch_size])\n",
      " \n",
      "  #train_inputs = train_data[:num_unrollings]\n",
      "  #train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
      "  #tf.cast(train_labels,tf.float64)\n",
      "    \n",
      "  tf_train_labels_float = []\n",
      "  for i,_ in enumerate(labels):\n",
      "      #print (type(train_labels[i]))\n",
      "      tf_train_labels_float.append(tf.cast(tf_train_labels[i],tf.float32))\n",
      "        \n",
      "  tf_valid_dataset = list()\n",
      "  tf_valid_mask = list()\n",
      "  for i in range(num_unrollings):\n",
      "        #tf_valid_dataset.append(tf.constant(valid_dataset[:,i]))\n",
      "        tf_valid_dataset.append(tf.placeholder(tf.int64, shape=[valid_dataset.shape[0]]))\n",
      "        tf_valid_mask.append(tf.placeholder(tf.float32, shape=[valid_dataset.shape[0]]))\n",
      "#========================================#            \n",
      "        \n",
      "#===============\u6d4b\u8bd5\u96c6\u6570\u636e\u8f93\u5165==============#\n",
      "  tf_test_dataset = list()\n",
      "  tf_test_mask = list()\n",
      "  for i in range(num_unrollings):\n",
      "        tf_test_dataset.append(tf.placeholder(tf.int64, shape=[test_dataset.shape[0]]))#[batch_size,vocabulary_size]))\n",
      "        tf_test_mask.append(tf.placeholder(tf.float32, shape=[test_dataset.shape[0]]))#[batch_size,vocabulary_size]))\n",
      "        \n",
      "#========================================#                    \n",
      "\n",
      "        \n",
      "  logits = model(tf_train_inputs,mask = tf_train_mask,dropoutFlag = True)\n",
      "  # State saving across unrollings.\n",
      "  #with tf.control_dependencies([saved_output.assign(output),\n",
      "  #                                 saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "  loss = tf.reduce_mean(\n",
      "      tf.nn.softmax_cross_entropy_with_logits(\n",
      "        logits, tf.one_hot(indices = tf.concat(0, tf_train_labels),depth = kindNumLabels,on_value = 1.0,off_value = 0.0)))\n",
      "  \n",
      "  # Optimizer.\n",
      "  global_step = tf.Variable(0)\n",
      "  learning_rate = tf.train.exponential_decay(\n",
      "    10.0, global_step, 5000, 0.1, staircase=True)\n",
      "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)#\u8fd9\u4e0b\u9762\u7684\u51e0\u884c\u5e72\u5565\u7528\u7684\uff1f\n",
      "  gradients, v = zip(*optimizer.compute_gradients(loss))#\u8fd9\u91cc\u7c7b\u4f3c\u4e8e\u5c06\u4e0d\u540c\u503c\u7684\u68af\u5ea6\u503c\u5168\u90e8unpack\uff0c\u7136\u540e\u5206\u522b\u653e\u5230gradients\u548cv\u4e24\u4e2a\u5217\u8868\u91cc\u9762\n",
      "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)#\u8fd9\u91cc\u662f\u589e\u52a0\u6b63\u5219\u9879\n",
      "  optimizer = optimizer.apply_gradients(\n",
      "    zip(gradients, v), global_step=global_step)\n",
      "\n",
      "  # Predictions.\n",
      "  train_prediction = tf.nn.softmax(logits)\n",
      "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,mask = tf_valid_mask,chooseState = \"valid\"))\n",
      "  test_prediction = tf.nn.softmax(model(tf_test_dataset,mask = tf_test_mask,chooseState = \"test\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_plot_x = []\n",
      "train_plot_y = []\n",
      "valid_plot_y = []\n",
      "\n",
      "num_steps = 3001\n",
      "\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "  tf.initialize_all_variables().run()\n",
      "  print('Initialized')\n",
      "  for step in range(num_steps):\n",
      "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
      "    batch_mask = train_mask[offset:(offset + batch_size), :]\n",
      "    feed_dict = dict()\n",
      "    feed_dict[tf_train_labels] = batch_labels\n",
      "\n",
      "    \n",
      "    for i in range(num_unrollings):\n",
      "      feed_dict[tf_train_inputs[i]] = batch_data[:,i]   \n",
      "      feed_dict[tf_train_mask[i]] = batch_mask[:,i]   \n",
      "      \n",
      "      feed_dict[tf_valid_dataset[i]] = valid_dataset[:,i]           \n",
      "      feed_dict[tf_valid_mask[i]] = valid_mask[:,i]   \n",
      "        \n",
      "      feed_dict[tf_test_dataset[i]] = test_dataset[:,i]   \n",
      "      feed_dict[tf_test_mask[i]] = test_mask[:,i]   \n",
      "        \n",
      "    _, l, predictions, lr = session.run(\n",
      "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
      "    \n",
      "\n",
      "\n",
      "    if (step % 50 == 0):\n",
      "      print('Minibatch loss at step %d: %f' % (step, l))\n",
      "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
      "      train_plot_x.append(step)\n",
      "      train_plot_y.append(accuracy(predictions, batch_labels))\n",
      "       \n",
      "      print('Validation accuracy: %.1f%%' % accuracy(\n",
      "        valid_prediction.eval(feed_dict = feed_dict), valid_labels))\n",
      "      valid_plot_y.append(accuracy(valid_prediction.eval(feed_dict = feed_dict), valid_labels))\n",
      "       \n",
      "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(feed_dict = feed_dict), test_labels))\n",
      "  classificationReport = classification_report(test_labels, np.argmax(test_prediction.eval(feed_dict = feed_dict),1))\n",
      "  plot_classification_report(classificationReport)\n",
      "  plt.savefig('cnn_glove_plot_classif_report.png', dpi=200, format='png', bbox_inches='tight')\n",
      "  plt.close()\n",
      "        \n",
      "  plt.plot(train_plot_x, train_plot_y,'b-',label = \"train accuracy\")\n",
      "  plt.plot(train_plot_x, valid_plot_y,'r-',label = \"valid accuracy\")\n",
      "  plt.xlabel(\"step\")\n",
      "  plt.ylabel(\"accuracy\")\n",
      "  plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Minibatch loss at step 0: 1.117818"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 30.5%\n",
        "Validation accuracy: 49.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 50: 2.639998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 57.8%\n",
        "Validation accuracy: 42.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 100: 3.232611"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 45.3%\n",
        "Validation accuracy: 33.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 150: 1.007990"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 57.8%\n",
        "Validation accuracy: 48.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 200: 0.887156"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 61.7%\n",
        "Validation accuracy: 48.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 250: 0.832149"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 68.8%\n",
        "Validation accuracy: 50.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 300: 1.346855"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 51.6%\n",
        "Validation accuracy: 47.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 350: 1.628042"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 58.6%\n",
        "Validation accuracy: 49.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 400: 1.446895"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 64.1%\n",
        "Validation accuracy: 55.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 450: 0.781739"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 71.1%\n",
        "Validation accuracy: 55.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 500: 0.622405"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 71.9%\n",
        "Validation accuracy: 54.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 550: 0.403920"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.9%\n",
        "Validation accuracy: 57.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 600: 0.659685"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 76.6%\n",
        "Validation accuracy: 52.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 650: 0.459420"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 78.1%\n",
        "Validation accuracy: 56.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 700: 0.251515"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 92.2%\n",
        "Validation accuracy: 56.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 750: 0.233544"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 55.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 800: 0.166301"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 55.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 850: 0.207056"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 92.2%\n",
        "Validation accuracy: 56.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 900: 0.222305"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 94.5%\n",
        "Validation accuracy: 56.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 950: 0.152158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 56.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1000: 0.109669"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.1%\n",
        "Validation accuracy: 54.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1050: 0.161406"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 92.2%\n",
        "Validation accuracy: 53.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1100: 0.108247"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 54.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1150: 0.068572"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 98.4%\n",
        "Validation accuracy: 54.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1200: 0.115426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 53.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1250: 0.110270"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 54.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1300: 0.055112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 53.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1350: 0.070508"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 97.7%\n",
        "Validation accuracy: 54.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1400: 0.030627"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1450: 0.052075"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 54.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1500: 0.027642"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 55.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1550: 0.016934"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 52.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1600: 0.026236"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 54.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1650: 0.031963"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 54.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1700: 0.030257"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 56.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1750: 0.038311"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 97.7%\n",
        "Validation accuracy: 53.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1800: 0.057396"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 53.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1850: 0.014746"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1900: 0.009557"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1950: 0.014238"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 54.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2000: 0.012144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 52.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2050: 0.007727"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 55.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2100: 0.008388"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2150: 0.008967"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 53.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2200: 0.032681"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 98.4%\n",
        "Validation accuracy: 54.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2250: 0.008535"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2300: 0.005022"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2350: 0.018205"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 99.2%\n",
        "Validation accuracy: 53.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2400: 0.005582"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2450: 0.005880"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2500: 0.001860"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2550: 0.002785"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2600: 0.009426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2650: 0.006144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2700: 0.003102"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2750: 0.003210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2800: 0.009278"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2850: 0.004764"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 56.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2900: 0.003276"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 54.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2950: 0.003131"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 3000: 0.001386"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 53.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test accuracy: 58.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.7, 0.76, 0.73]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.52, 0.48, 0.5]\n",
        "[0.39, 0.31, 0.35]\n",
        "plotMat: [[0.7, 0.76, 0.73], [0.52, 0.48, 0.5], [0.39, 0.31, 0.35]]\n",
        "support: [308, 215, 77]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}